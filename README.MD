

Данный проект посвящен сравнительному анализу различных алгоритмов машинного обучения для решения задачи бинарной классификации: предсказание наличия сердечно-сосудистого заболевания у пациента.

Описание датасета

Используется датасет [Heart Disease Dataset](https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset) с Kaggle. Он содержит 1025 записей и 14 признаков, включая возраст, пол, тип боли в груди, артериальное давление, уровень холестерина и другие клинические показатели. Целевая переменная (`target`) бинарна: 0 — отсутствие болезни, 1 — наличие болезни.

Задачи проекта

1.  Загрузить и провести первичный анализ данных.
2.  Обучить и оценить качество нескольких моделей классификации.
3.  Сравнить эффективность моделей и сделать выводы.

 Используемые модели

В работе были реализованы и обучены следующие модели:

*   **Gradient Boosting Classifier:** Ансамблевый метод на основе градиентного бустинга.
*   **AdaBoost Classifier:** Ансамблевый метод, использующий "пни" (деревья глубины 1).
*   **Quadratic Discriminant Analysis (QDA):** Метод, основанный на предположении о нормальном распределении признаков с разными ковариационными матрицами для классов.
*   **K-Nearest Neighbors (kNN):** Метод ближайших соседей. Для него был проведен подбор оптимального количества соседей `k` с помощью `GridSearchCV`.
*   **Dummy Classifier:** Простейший классификатор, выдающий случайный результат (стратегия `uniform`). Используется как baseline для сравнения.

Основные этапы работы

1.  **Загрузка и предобработка:**
    *   Загрузка данных через Kaggle API.
    *   Ознакомление со структурой данных, типами признаков и базовой статистикой (`df.info()`, `df.describe()`).
    *   Для модели kNN выполнена стандартизация данных с помощью `StandardScaler`.
    *   Разделение данных на обучающую и тестовую выборки (с параметром `random_state=42` для воспроизводимости результатов).

2.  **Обучение и оценка моделей:**
    *   Все модели обучаются на тренировочных данных.
    *   Для kNN оптимальное значение `k` (равное 1) подбирается с помощью кросс-валидации.
    *   Качество каждой модели оценивается на тестовой выборке по метрикам точности (`accuracy`) и с помощью `classification_report` (precision, recall, f1-score).

3.  **Визуализация:**
    *   Для наглядной демонстрации принципов работы классификаторов построены графики решающих поверхностей на плоскости двух признаков (`age` и `sex`). Визуализация выполнена для двух моделей: оптимальной (kNN) и "глупой" (Dummy Classifier).

Результаты и выводы

| Модель | Accuracy | Precision (0/1) | Recall (0/1) | F1-score (0/1) |
| :--- | :--- | :--- | :--- | :--- |
| **kNN (k=1)** | **0.990** | 0.98 / 1.00 | 1.00 / 0.98 | 0.99 / 0.99 |
| Gradient Boosting | 0.95 | 0.96 / 0.95 | 0.95 / 0.95 | 0.95 / 0.95 |
| QDA | 0.82 | 0.91 / 0.73 | 0.74 / 0.91 | 0.82 / 0.81 |
| AdaBoost | 0.81 | 0.88 / 0.76 | 0.74 / 0.89 | 0.81 / 0.82 |
| **Dummy (baseline)** | **0.49** | 0.51 / 0.48 | 0.45 / 0.54 | 0.48 / 0.51 |

*   **Лучший результат** показала модель k-Nearest Neighbors с одним соседом (`k=1`), достигнув практически идеальной точности в 99%.
*   **Высокое качество** градиентного бустинга (95%) подтверждает эффективность ансамблевых методов на структурированных данных.
*   **Важное замечание:** Модель kNN с `k=1` имеет высокий риск **переобучения**. Это означает, что она очень чувствительна к шуму и выбросам в данных и может хуже работать на новых, незнакомых примерах.
*   **Ожидаемо низкий результат** Dummy-классификатора (49%, близко к случайному угадыванию) подтверждает, что "умные" модели действительно находят закономерности в данных, а не угадывают результат.
*   AdaBoost и QDA показали сопоставимые результаты (около 81-82%), что значительно лучше случайного угадывания, но уступают kNN и градиентному бустингу.

**Итог:** Для данной задачи лучше всего подходят методы, основанные на близости объектов (kNN) и градиентном бустинге. Однако для практического применения модели kNN с `k=1` рекомендуется провести её дополнительное тестирование на новых данных, чтобы исключить возможность переобучения.